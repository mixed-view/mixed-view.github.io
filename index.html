<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Mixed-View Panorama Synthesis: A novel approach for synthesizing panoramas using both satellite imagery and ground-level panoramas">
  <meta property="og:title" content="Mixed-View Panorama Synthesis using Geospatially Guided Diffusion"/>
  <meta property="og:description" content="Synthesizing novel panoramas by combining satellite imagery and ground-level panoramas using diffusion-based modeling"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/img/simple_framework8.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Mixed-View Panorama Synthesis using Geospatially Guided Diffusion">
  <meta name="twitter:description" content="A novel approach for synthesizing panoramas using both satellite imagery and ground-level panoramas">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/img/simple_framework8.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="panorama synthesis, mixed-view, satellite imagery, diffusion models, cross-view synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mixed-View Panorama Synthesis using Geospatially Guided Diffusion</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mixed-View Panorama Synthesis</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://steven-xiong.github.io" target="_blank">Zhexiao Xiong</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://xtrigold.github.io/" target="_blank">Xin Xing</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.scottworkman.com/" target="_blank">Scott Workman</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://subash-khanal.github.io/" target="_blank">Subash Khanal</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jacobsn.github.io/" target="_blank">Nathan Jacobs</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
              <span class="author-block"><sup>1</sup>Washington University in St. Louis,</span>
              <span class="author-block"><sup>2</sup>University of Nebraska Omaha,</span>
              <span class="author-block"><sup>3</sup>DZYNE Technologies</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="is-size-5 publication-authors" style="margin-top: 10px;">
              <span class="author-block"><b>Transactions on Machine Learning Research(TMLR), 2025</b></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2407.09672.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.09672" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
              </span>
              
              <!-- supplementary -->
              <!-- </span>
                  <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>

              
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/img/simple_framework8.jpg" alt="Mixed-view panorama synthesis framework"/>
        <h2 class="subtitle has-text-centered">
          We propose a new task, mixed-view panorama synthesis, 
          in which a satellite image and a set of nearby panoramas (blue, yellow, and green) are used to render a panorama at a novel location (red). 
          Our approach uses diffusion-based modeling and attention to enable flexible, multimodal control.
        </h2>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce the task of mixed-view panorama synthesis, where the goal is to synthesize a novel panorama given a small set of input panoramas and a satellite image of the area. 
            This contrasts with previous work which only uses input panoramas (same-view synthesis), or an input satellite image (cross-view synthesis). 
            We argue that the mixed-view setting is the most natural to support panorama synthesis for arbitrary locations worldwide. 
            A critical challenge is that the spatial coverage of panoramas is uneven, with few panoramas available in many regions of the world. 
            We introduce an approach that utilizes diffusion-based modeling and an attention-based architecture for extracting information from all available input imagery. 
            Experimental results demonstrate the effectiveness of our proposed method. In particular, our model can handle scenarios when the available panoramas are sparse or far from the location of the panorama we are attempting to synthesize.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="text-align: center;">Methods</h2>
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/img/local_attention_ECCV_3.6.jpg" alt="Local attention mechanism"/>
        <h2 class="subtitle has-text-centered">
          Our local attention mechanism extracts features from nearby panoramas to provide contextual information for synthesis. 
          The attention weights are learned based on spatial proximity and visual similarity.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/img/global_attention_extraction.jpg" alt="Global attention extraction"/>
        <h2 class="subtitle has-text-centered">
          Global attention extraction from satellite imagery provides overhead context and spatial structure. 
          This enables our model to understand the overall layout and generate panoramas consistent with the surrounding environment.
       </h2>
     </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/img/fusion_ECCV1.jpg" alt="Feature fusion module"/>
        <h2 class="subtitle has-text-centered">
          Our fusion module combines features from both satellite and panorama inputs through a learned attention mechanism. 
          This allows the model to leverage complementary information from both modalities for high-quality panorama synthesis.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            Our mixed-view panorama synthesis approach consists of three main components:
            (1) A satellite image encoder that extracts global spatial features,
            (2) A panorama encoder that processes nearby ground-level views, and
            (3) A diffusion-based synthesis module that generates novel panoramas by combining both sources of information.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/img/framework_base_tmlr1.jpg" alt="Overall framework"/>
        <h2 class="subtitle has-text-centered">
          Overview of our mixed-view panorama synthesis framework. 
          The model takes as input a satellite image and a set of nearby panoramas, 
          and generates a novel panorama at the target location using diffusion-based synthesis.
        </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/img/local_attention_ECCV.jpg" alt="Local attention details"/>
        <h2 class="subtitle has-text-centered">
          Visualization of local geospatial attention. 
          The target location is represented by a green square in the satellite image. 
          The nearby street-level panoramas (color-coded borders) are represented by same-colored circles in the satellite image.
        </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/img/global_att.jpg" alt="Global attention mechanism"/>
        <h2 class="subtitle has-text-centered">
          Visualization of global geospatial attention. 
          The color-coded attention maps for two target locations are shown, corresponding to the same-colored dots in the satellite image. 
          Darker colors represent more salient regions.
        </h2>
    </div>
  </div>
</section>


<!-- Method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our approach on diverse locations worldwide, demonstrating its ability to synthesize realistic panoramas 
            even when input panoramas are sparse or distant from the target location. Our method significantly outperforms 
            baseline approaches that use only satellite imagery or only ground-level panoramas.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3" style="text-align: center;">Comparison with Baselines</h2>
      <div class="columns is-multiline">
        <div class="column is-2">
          <img src="static/img/results/sat1.jpg" alt="Satellite Input" style="height: 300px; width: 50%; object-fit: cover;"/>
          <p class="has-text-centered"><b>Satellite Input</b></p>
        </div>
        <div class="column is-2">
          <img src="static/img/results/pix2pix_aligned.jpg" alt="Pix2Pix" style="height: 300px; width: 100%; object-fit: cover;"/>
          <p class="has-text-centered"><b>Pix2Pix</b></p>
        </div>
        <div class="column is-2">
          <img src="static/img/results/panogan_aligned.jpg" alt="PanoGAN" style="height: 300px; width: 100%; object-fit: cover;"/>
          <p class="has-text-centered"><b>PanoGAN</b></p>
        </div>
        <div class="column is-2">
          <img src="static/img/results/sat2density_aligned.jpg" alt="Sat2Density" style="height: 300px; width: 100%; object-fit: cover;"/>
          <p class="has-text-centered"><b>Sat2Density</b></p>
        </div>
        <div class="column is-2">
          <img src="static/img/results/ours_new1.jpg" alt="Ours" style="height: 300px; width: 100%; object-fit: cover;"/>
          <p class="has-text-centered"><b>Ours</b></p>
        </div>
        <div class="column is-2">
          <img src="static/img/results/gt1.jpg" alt="Ground Truth" style="height: 300px; width: 100%; object-fit: cover;"/>
          <p class="has-text-centered"><b>Ground Truth</b></p>
        </div>
      </div>
      <h2 class="subtitle has-text-centered" style="margin-top: 20px;">
        Comparison with baseline methods. The cross-view synthesis methods that we compare with are trained on our collected center-aligned satellite images. 
        Our approach, which integrates nearby street-level panoramas, not only generates more realistic results when compared to baselines, but more accurate results both semantically and geometrically when compared to the ground truth.
      </h2>
    </div>
  </div>
</section>

<!-- Experimental Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our approach on diverse locations worldwide, demonstrating its ability to synthesize realistic panoramas 
            even when input panoramas are sparse or distant from the target location. Our method significantly outperforms 
            baseline approaches that use only satellite imagery or only ground-level panoramas.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Ablation Study -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3" style="text-align: center;">Ablation Study</h2>
      <img src="static/img/ablation_new_ECCV.jpg" alt="Ablation study results"/>
      <h2 class="subtitle has-text-centered" style="margin-top: 20px;">
        Ablation study showing the effectiveness of different components in our mixed-view panorama synthesis approach. 
        Our method effectively combines information from both satellite imagery and ground-level panoramas to generate realistic novel views.
      </h2>
    </div>
  </div>
</section>


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            
            <source src="static/images/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xiong2024mixed,
  title={Mixed-View Panorama Synthesis using Geospatially Guided Diffusion},
  author={Xiong, Zhexiao and Xing, Xin and Workman, Scott and Khanal, Subash and Jacobs, Nathan},
  journal={arXiv preprint arXiv:2407.09672},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
             <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
